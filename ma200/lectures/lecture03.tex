\lecture{2024-08-07}{Big-Oh and matrix norms}
On $\R^n$, we will always fix the $\ell^2$-norm

\begin{notation}
    \[
        L(\R^n, \R^m) = \sset{T\colon \R^n \to \R^m}{T \text{ is linear}}
    \] and \[
        M_{m \times n}(\R) \cong L(\R^n, \R^m)
    \] using the isomorphism $A \mapsto T_A$ where \begin{align*}
    T_A\colon \R^n &\to \R^m \\
          v &\mapsto Av,
    \end{align*} where $v$ is interpreted as a column vector.
    We will also write $L(\R^n)$ for $L(\R^n, \R^n)$.
\end{notation}

\begin{definition*}[liminf and limsup] \label{def:liminf-limsup}
    Let $f\colon X \to \R$ be a function on a topological space $X$.
    We define the \emph{limit inferior} and \emph{limit superior} of $f$ as
    \begin{align*}
        \liminf_{x \to a} f(x) &= \sup_V \inf_{x \in V} f(x) \\
        \limsup_{x \to a} f(x) &= \inf_V \sup_{x \in V} f(x)
    \end{align*} where $V$ ranges over all open neighbourhoods of $a$
    that contain at least one point other than $a$.
\end{definition*}
\begin{exercise*}[self]
    Let $(X, d)$ be a connected metric space with at least two points.
    Then \begin{align*}
        \liminf_{x \to a} f(x)
            &= \lim_{\eps \searrow 0}\; \inf_{0 < d(x, a) < \eps} f(x) \\
        \limsup_{x \to a} f(x)
            &= \lim_{\eps \searrow 0}\; \sup_{0 < d(x, a) < \eps} f(x)
    \end{align*}
\end{exercise*}
\begin{proof}
    We first need to show that
    $\sset{x \in X}{0 < d(x, a) < \eps}$ is non-empty
    for each $\eps > 0$.
    Suppose this were not the case for some $\eps$.
    Then $B(a, \eps) = \set a = \wbar{B(a, \eps/2)}$ is clopen,
    contradicting the connectedness of $X$.

    Notice that for any $\eps_1 > \eps_2 > 0$,
    \[
        \sset{x \in X}{0 < d(x, a) < \eps_1}
        \subseteq \sset{x \in X}{0 < d(x, a) < \eps_2},
    \] so the infimum increases as $\eps \searrow 0$.
\end{proof}

\begin{definition*}[$O$ notation] \label{def:o}
    Let $f\colon \R^n \to \R^m$ and $g\colon \R^n \to \R^k$.
    We say that
    \begin{enumerate}
        \item $f(x) = o(g(x))$ as $x \to a$ if \[
            \lim_{x \to a} \frac{\norm{f(x)}}{\norm{g(x)}} = 0,
        \]
        \item $f(x) = O(g(x))$ as $x \to a$ if \[
            \limsup_{x \to a} \frac{\norm{f(x)}}{\norm{g(x)}} < \infty.
        \]
    \end{enumerate}
    where the assumption is that $g$ is non-zero in some
    neighbourhood of $a$.
\end{definition*}

\begin{exercise}
    Show that the definition of $O$ is equivalent to the following:

    We say that $f(x) = O(g(x))$ as $x \to a$ if there exists an open
    neighbourhood $V$ of $a$ such that $\frac{\norm{f(x)}}{\norm{g(x)}}$
    is bounded on $V$.
\end{exercise}
\begin{solution}
    Call the ratio $h$.
    \begin{align*}
        \inf_V \sup_{x \in V} h(x) \le \infty
            &\iff \exists V (\sup_{x \in V} h(x) < \infty) \\
            &\iff \exists V \exists M (\forall x \in V, h(x) \le M) \qedhere
    \end{align*}
\end{solution}

\begin{exercise}
    If $f_1, f_2 = O(g)$ then $f_1 \pm f_2 = O(g)$.
    If $f_1, f_2 = o(g)$ then $f_1 \pm f_2 = o(g)$.
\end{exercise}
\begin{solution}
    It do be a self-evident truth.
\end{solution}

\section{Matrix norms} \label{sec:matrix-norms}

\begin{definition*}[Hilbert-Schmidt norm] \label{def:hsn}
    For a matrix $A \in M_{m \times n}(\R)$, we define the
    \emph{Hilbert-Schmidt} or \emph{Frobenius} norm by \[
        \hsn A = \Big(\sum_{i,j} a_{ij}^2\Big)^{1/2}
    \]
\end{definition*}

\begin{exercise}
    Show that $\hsn A^2 = \Tr(A^\T A) = \Tr(AA^\T)$.
\end{exercise}
\begin{solution}
    \begin{align*}
        (A^\T A)_{i i} &= \sum_k (A^\T)_{i k} A_{k i} \\
            &= \sum_k a_{k i}^2 \\
        \implies \Tr(A^\T A) &= \sum_i \sum_k a_{k i}^2 \\
            &= \hsn A^2.
    \end{align*}
    Since $\hsn A = \hsn{A^\T}$, we also have $\Tr(AA^\T) = \hsn A^2$.
\end{solution}
\begin{proposition}
    Any linear transformation is continuous.
\end{proposition}
\begin{proof}
    Let $T \in L(\R^n, \R^m)$.
    Then \begin{align*}
        \norm{Tx} &= \norm*{T\ab(\sum x_i e_i)} \\
                 &= \norm*{\sum x_i Te_i} \\
                 &\le \sum \abs{x_i} \norm{T e_i} \\
                 &\le \norm{x} \sum \norm{T e_i} \\
                 &= M \norm{x} \yesnumber \label{eq:operator_norm}
    \end{align*} where $M = \norm{Te_1} + \cdots + \norm{Te_n}$.

    Now \[
        \norm{Tx - Ty} = \norm{T(x - y)} \le M \norm{x - y}
    \] says that $T$ is Lipschitz continuous with Lipschitz constant $M$.
\end{proof}

We temporarity define two norms on $M_{m \times n}(\R)$:
\begin{align*}
    {\norm T}_S &= \sup_{\norm{x} = 1} \norm{Tx} \\
    {\norm T}_B &= \sup_{\norm{x} \le 1} \norm{Tx}
\end{align*}
\begin{lemma} \label{lem:operator_norm}
    ${\norm T}_S = {\norm T}_B$.
\end{lemma}
\begin{proof}
    From the definition it is obvious that ${\norm T}_S \le {\norm T}_B$.
    Now for any $x \in \R^n \setminus \set{0}$, let $y = x/\norm{x}$.
    \begin{align*}
        \norm{Ty} &\le {\norm T}_S \\
        \frac{\norm{Tx}}{\norm{x}} &\le {\norm T}_S \\
        \implies \norm{Tx} &\le {\norm T}_S \norm{x}
    \end{align*} Thus for $\norm{x} \le 1$,
    we have $\norm{Tx} \le {\norm T}_S$ (check $0$ separately).
    So ${\norm T}_B \le {\norm T}_S$.
\end{proof}

\begin{definition*}[operator norm] \label{def:operator_norm}
    For any $T \in L(\R^n, \R^m)$, we define the \emph{operator norm} by \[
        \norm{T} = \sup_{\norm{x} = 1} \norm{Tx}
    \]
\end{definition*}
From the previous lemma, we can also write
\[
    \norm{T} = \sup_{\norm{x} \le 1} \norm{Tx}
        = \sup_{x \ne 0} \frac{\norm{Tx}}{\norm{x}}.
\]
From \cref{eq:operator_norm}, we have \[
    \norm{T} \le \norm{Te_1} + \cdots + \norm{Te_n}.
\] So the operator norm is finite.

\begin{proposition}
    The operator norm is a norm on $L(\R^n, \R^m)$.
\end{proposition}
\begin{proof}
    Let $T, S \in L(\R^n, \R^m)$.
    \begin{enumerate}[label=\small(N\arabic*)]
        \item Positivity is by positivity of the vector norm.
        \item Suppose $T$ is not identically zero.
            Let $v \ne 0$ be such that $\norm{Tv} \ne 0$.
            Then \[
                \norm T = \sup_{x \ne 0} \frac{\norm{Tx}}{\norm{x}}
                    \ge \frac{\norm{Tv}}{\norm{v}}
                    > 0.
            \]
        \item $\norm{\lambda T} = \sup_{\norm{x} = 1} \norm{\lambda Tx}
            = \abs{\lambda} \sup_{\norm{x} = 1} \norm{Tx}
            = \abs{\lambda} \norm{T}$.
        \item \begin{align*}
            \norm{T + S} &= \sup_{\norm{x} = 1} \norm{(T + S)x} \\
                &\le \sup_{\norm{x} = 1} \norm{Tx} + \norm{Sx} \\
                &\le \sup_{\norm{x} = 1} \norm{Tx}
                        + \sup_{\norm{x} = 1} \norm{Sx} \\
                &= \norm{T} + \norm{S}. \qedhere
        \end{align*}
    \end{enumerate}
\end{proof}

\begin{proposition}
    Let $T_2 \in L(\R^m, \R^n)$ and $T_1 \in L(\R^n, \R^k)$.
    Then \[
        \norm{T_1 \circ T_2} \le \norm{T_1} \norm{T_2}
    \]
\end{proposition}
\begin{proof}
    Let $x \in \R^m$ with $\norm{x} = 1$.
    Then \[
        \norm{T_1T_2 x} \le \norm{T_1} \norm{T_2 x}
                    \le \norm{T_1} \norm{T_2}. \qedhere
    \]
\end{proof}

Since $M_{m \times n}(\R) \cong \R^{mn}$, we can conclude that
the Hilbert-Schmidt norm and the operator norm are equivalent,
as are any two norms on $M_{m \times n}(\R)$.
Thus we can talk about openness and continuity without specifying
the norm.
\Crefifdef{prb:operator-hs}{Problem 10 on assignment 1} discusses
their equivalence with specific bounds.

\begin{proposition*}
    $\GL_n(\R)$ is open in $\M_n(\R)$.
\end{proposition*}
\begin{proof}
    $\det \colon \M_n(\R) \to \R$ is continuous because
    it is a polynomial in the entries of the matrix.
    Note that $\GL_n(\R) = \det^{-1}(\R \setminus \set{0})$,
    so it is the preimage of an open set, which is open
    by \cref{prp:cont-open}.
\end{proof}
Determinants pose a problem in infinite dimensions.
This also doesn't provide estimates on the size of the neighbourhood.
We will go through Rudin's proof in \cref{thm:inv-cont-rudin}
which avoids determinants.

We need to figure out a special case first: what ball around the identity
matrix is fully invertible?
A reasonable guess for the radius is $1$ (intuiting from the $1$D case).
\begin{lemma*} \label{thm:gln-open-i}
    The open ball of radius $1$ around $I$ in $\M_n(\R)$
    is contained in $\GL_n(\R)$.
\end{lemma*}
\begin{proof}
    Let $X \in \M_n(\R)$ with $\norm{X - I} < 1$.

    Let $v \in \R^n \setminus \set 0$.
    Then $\norm{(X - I)v} < \norm v$ implies that $(X - I)v \ne v$.
    Thus $Xv \ne 0$ and so $X$ is invertible.
\end{proof}
This will also be useful in \cref{thm:inv-cont-rudin}.
This can also be proven by borrowing the following result from $\C$. \[
    \frac1{1-z} = 1 + z + z^2 + \ldots \quad \text{for} \quad \abs{z} < 1.
\]
(This was the first thought a student had when prompted.)
We approach it this way as well, since this gives us an explicit inverse.
\begin{lemma*} \label{thm:inv-series}
    Let $Z \in \M_n(\R)$ be such that $\norm{Z} < 1$.
    Then
    \begin{enumerate}
        \item $\sum_{n=0}^\infty Z^n$ converges.
        \item $I - Z$ is invertible.
        \item $(I - Z)^{-1} = \sum_{n=0}^\infty Z^n$.
    \end{enumerate}
\end{lemma*}
\begin{proof}
    By $\norm{AB} \le \norm A \norm B$, $\norm{Z^k} \le \norm Z^k$.

    It is easy to see that the series converges by the Cauchy criterion.
    For any $\eps > 0$, there is some $n$ such that \[
        \norm*{\sum_{k=n}^m Z^k} \le \sum_{k=n}^m \norm{Z^k} < \eps
    \]
    for all $m > n$.

    Now let $S_n = \sum_{k=0}^n Z^k$ and
    $S_\infty = \lim_{n \to \infty} S_n$.
    Then $(I - Z)S_n = I - Z^{n+1}$ and so
    $(I - Z)S_n \to I$ as $n \to \infty$.
    Since matrix multiplication is continuous, we can take the limit
    inside the product and get $(I - Z)S_\infty = I$.
\end{proof}
\begin{remark}
    For infinite-dimensional spaces, we also need to show
    $S_\infty (I - Z) = I$, which will be done in the exact same way.
\end{remark}

\begin{proposition*} \label{thm:inv-cont}
    $A \mapsto A^{-1}$ is continuous on $\GL_n(\R)$.
\end{proposition*}
\begin{proof}
    Let $A \in \GL_n(\R)$.
    Then $A^{-1} = \frac1{\det A} \adj A$.
    Each entry of $A^{-1}$ is a rational function in the entries of $A$,
    so $A \mapsto A^{-1}$ is continuous by \cref{thm:cont:Rn}.
\end{proof}
